{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11472097,"sourceType":"datasetVersion","datasetId":7189537},{"sourceId":11641302,"sourceType":"datasetVersion","datasetId":7304749},{"sourceId":11650320,"sourceType":"datasetVersion","datasetId":7311138},{"sourceId":11766806,"sourceType":"datasetVersion","datasetId":7387101},{"sourceId":11787031,"sourceType":"datasetVersion","datasetId":7400812},{"sourceId":11787267,"sourceType":"datasetVersion","datasetId":7400986},{"sourceId":11788095,"sourceType":"datasetVersion","datasetId":7401616},{"sourceId":11789125,"sourceType":"datasetVersion","datasetId":7402316},{"sourceId":389488,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":320861,"modelId":341456},{"sourceId":389534,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":320895,"modelId":341493},{"sourceId":389621,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":320953,"modelId":341559},{"sourceId":389820,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321092,"modelId":341702},{"sourceId":390112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321329,"modelId":341934},{"sourceId":390126,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321341,"modelId":341945},{"sourceId":390288,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321464,"modelId":342067},{"sourceId":390462,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321602,"modelId":342208},{"sourceId":390953,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321958,"modelId":342580},{"sourceId":390965,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":321968,"modelId":342589}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:22:02.876311Z","iopub.execute_input":"2025-05-03T19:22:02.876616Z","iopub.status.idle":"2025-05-03T19:22:02.881533Z","shell.execute_reply.started":"2025-05-03T19:22:02.876584Z","shell.execute_reply":"2025-05-03T19:22:02.88065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nmetadata_path = \"/kaggle/input/abo-small/metadata/images.csv\"\ndf = pd.read_csv(metadata_path)\ndf.head()\nprint(\"Total images:\", len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:22:02.882459Z","iopub.execute_input":"2025-05-03T19:22:02.882765Z","iopub.status.idle":"2025-05-03T19:22:03.596056Z","shell.execute_reply.started":"2025-05-03T19:22:02.882736Z","shell.execute_reply":"2025-05-03T19:22:03.595158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Loading the listings\nwith open(\"/kaggle/input/abo-product-metadata/combined_listings.json\", \"r\") as f:\n    data = json.load(f)\n\nprint(f\"Total entries: {len(data)}\")\n\n# # Printing a sample listing\n# sample_id = list(data.keys())[0]\n# print(\"Sample image_id:\", sample_id)\n# print(\"Associated product data:\", data[sample_id])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:22:03.600578Z","iopub.execute_input":"2025-05-03T19:22:03.600857Z","iopub.status.idle":"2025-05-03T19:23:23.31708Z","shell.execute_reply.started":"2025-05-03T19:22:03.600838Z","shell.execute_reply":"2025-05-03T19:23:23.316005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Printing a sample listing\nsample_id = list(data.keys())[5]\nprint(\"Sample image_id:\", sample_id)\nprint(\"Associated product data:\", data[sample_id])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:23:23.318953Z","iopub.execute_input":"2025-05-03T19:23:23.31931Z","iopub.status.idle":"2025-05-03T19:23:23.393089Z","shell.execute_reply.started":"2025-05-03T19:23:23.319283Z","shell.execute_reply":"2025-05-03T19:23:23.392261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_metadata(data):\n    metadata_lines = []\n\n    def get_en_value(field):\n        \"\"\"Helper to extract English value from a language-tagged list.\"\"\"\n        return next((entry.get('value') for entry in data.get(field, [])\n                     if entry.get('language_tag', '').startswith('en') and entry.get('value')), None)\n\n\n    item_name = get_en_value('item_name')\n    if item_name:\n        metadata_lines.append(f\"Item Name: {item_name}\")\n\n    model_name = get_en_value('model_name')\n    if model_name:\n        metadata_lines.append(f\"Model Name: {model_name}\")\n        \n    brand = get_en_value('brand')\n    if brand:\n        metadata_lines.append(f\"Brand: {brand}\")\n\n\n    product_type = data.get('product_type', [{}])[0].get('value')\n    if product_type:\n        metadata_lines.append(f\"Product Type: {product_type}\")\n\n    style = get_en_value('style')\n    if style:\n        metadata_lines.append(f\"Style: {style}\")\n\n    color = get_en_value('color')\n    if color:\n        metadata_lines.append(f\"Color: {color}\")\n\n    # Weight\n    weight_data = data.get('item_weight', [{}])[0].get('normalized_value', {})\n    if weight_data:\n        weight = weight_data.get('value')\n        unit = weight_data.get('unit')\n        if weight is not None and unit:\n            metadata_lines.append(f\"Item Weight: {weight} {unit}\")\n\n    # Dimensions\n    dimensions = data.get('item_dimensions')\n    if dimensions:\n        height = dimensions.get('height', {}).get('value')\n        length = dimensions.get('length', {}).get('value')\n        width = dimensions.get('width', {}).get('value')\n        unit = dimensions.get('height', {}).get('unit')\n        if None not in (height, length, width, unit):\n            metadata_lines.append(f\"Item Dimensions (H x L x W): {height} x {length} x {width} {unit}\")\n\n    # Bullet Points (only English)\n    bullet_points = [\n        bp['value'] for bp in data.get('bullet_point', [])\n        if bp.get('language_tag', '').startswith('en') and bp.get('value')\n    ]\n    if bullet_points:\n        metadata_lines.append(\"Bullet Points: \" + \"; \".join(bullet_points))\n\n\n    item_keywords = get_en_value('item_keywords')\n    if item_keywords:\n        metadata_lines.append(f\"Item Keywords: {item_keywords}\")\n\n    return \"\\n\".join(metadata_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:23.113416Z","iopub.execute_input":"2025-05-03T19:24:23.113685Z","iopub.status.idle":"2025-05-03T19:24:23.125652Z","shell.execute_reply.started":"2025-05-03T19:24:23.113665Z","shell.execute_reply":"2025-05-03T19:24:23.124922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(extract_metadata(data[sample_id]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:25.566249Z","iopub.execute_input":"2025-05-03T19:24:25.56653Z","iopub.status.idle":"2025-05-03T19:24:25.572062Z","shell.execute_reply.started":"2025-05-03T19:24:25.566511Z","shell.execute_reply":"2025-05-03T19:24:25.570994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport random\nfrom collections import defaultdict\n\n\nprint(f\"Total entries in original data: {len(data)}\")\n\n# Helper to check if an entry is in English\ndef is_english(entry):\n    for field in ['brand', 'bullet_point', 'color', 'item_name', 'style', 'item_keywords']:\n        if field in entry:\n            values = entry[field]\n            if not all(v.get('language_tag', '').startswith('en') for v in values):\n                return False\n    return True\n\n# Filter to English-only entries\nenglish_entries = {\n    img_id: info for img_id, info in data.items() if is_english(info)\n}\n\nprint(f\"Entries after English filtering: {len(english_entries)}\")\n\n# Group by product type\nproduct_type_groups = defaultdict(list)\nfor img_id, info in english_entries.items():\n    product_type = info.get('product_type', [{}])[0].get('value')\n    if product_type:\n        product_type_groups[product_type].append((img_id, info))\n\n# Determine equal sample size per product type\nnum_types = len(product_type_groups)\ntarget_total = 30000\nper_type_quota = target_total // num_types\nsampled_data = {}\n\nprint(f\"Sampling approx {per_type_quota} entries per product type...\")\n\n# First round of balanced sampling\nfor ptype, items in product_type_groups.items():\n    sample_count = min(per_type_quota, len(items))\n    sampled = random.sample(items, sample_count)\n    for img_id, info in sampled:\n        sampled_data[img_id] = info\n\n# Fill remaining with random from types that had more than quota\nremaining = target_total - len(sampled_data)\noverflow_pool = []\n\nfor ptype, items in product_type_groups.items():\n    available = [item for item in items if item[0] not in sampled_data]\n    overflow_pool.extend(available)\n\nif remaining > 0 and overflow_pool:\n    additional_samples = random.sample(overflow_pool, min(remaining, len(overflow_pool)))\n    for img_id, info in additional_samples:\n        sampled_data[img_id] = info\n\nprint(f\"Final sampled size: {len(sampled_data)}\")\n\n# Save the sampled data\nwith open(\"sampled_english_equal_product_types.json\", \"w\") as f_out:\n    json.dump(sampled_data, f_out)\n\nprint(\"Saved to 'sampled_english_equal_product_types.json'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:27.548528Z","iopub.execute_input":"2025-05-03T19:24:27.548841Z","iopub.status.idle":"2025-05-03T19:24:40.704806Z","shell.execute_reply.started":"2025-05-03T19:24:27.548819Z","shell.execute_reply":"2025-05-03T19:24:40.703756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_base_path = \"/kaggle/input/abo-small/small\"\nmetadata_path = \"/kaggle/input/abo-small/metadata/images.csv\"\nmetadata_df = pd.read_csv(metadata_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:40.70636Z","iopub.execute_input":"2025-05-03T19:24:40.706645Z","iopub.status.idle":"2025-05-03T19:24:41.280287Z","shell.execute_reply.started":"2025-05-03T19:24:40.706625Z","shell.execute_reply":"2025-05-03T19:24:41.279446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install google-generativeai tqdm --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:41.281165Z","iopub.execute_input":"2025-05-03T19:24:41.281526Z","iopub.status.idle":"2025-05-03T19:24:47.378097Z","shell.execute_reply.started":"2025-05-03T19:24:41.281499Z","shell.execute_reply":"2025-05-03T19:24:47.37704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_key, val = None, None\nfor key, value in sampled_data.items():\n    sample_key = key\n    val = value\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:47.380364Z","iopub.execute_input":"2025-05-03T19:24:47.380686Z","iopub.status.idle":"2025-05-03T19:24:47.385858Z","shell.execute_reply.started":"2025-05-03T19:24:47.380641Z","shell.execute_reply":"2025-05-03T19:24:47.384655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metadata_df = metadata_df.set_index(\"image_id\")\n\n# Then retrieve path directly\npath = metadata_df.at[val.get(\"main_image_id\"), \"path\"]\n# print(\"Path:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:47.38672Z","iopub.execute_input":"2025-05-03T19:24:47.386997Z","iopub.status.idle":"2025-05-03T19:24:47.575402Z","shell.execute_reply.started":"2025-05-03T19:24:47.386973Z","shell.execute_reply":"2025-05-03T19:24:47.574481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_json(output):\n    try:\n        start = output.find('[')\n        end = output.rfind(']')\n        json_str = output[start:end+1]\n\n        data = json.loads(json_str)\n\n        # Check format\n        if not isinstance(data, list):\n            raise ValueError(\"The response should be a list of Q&A pairs.\")\n        for item in data:\n            if not isinstance(item, dict):\n                raise ValueError(\"Each item in the list should be a dictionary.\")\n            if \"question\" not in item or \"answer\" not in item:\n                raise ValueError(\"Each dictionary should contain 'question' and 'answer' keys.\")\n            if not isinstance(item[\"question\"], str) or not isinstance(item[\"answer\"], str):\n                raise ValueError(\"Both 'question' and 'answer' should be strings.\")\n\n        return data\n\n    except:\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:47.576372Z","iopub.execute_input":"2025-05-03T19:24:47.576623Z","iopub.status.idle":"2025-05-03T19:24:47.59859Z","shell.execute_reply.started":"2025-05-03T19:24:47.576603Z","shell.execute_reply":"2025-05-03T19:24:47.59754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_metadata(val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:24:47.599567Z","iopub.execute_input":"2025-05-03T19:24:47.599819Z","iopub.status.idle":"2025-05-03T19:24:47.62085Z","shell.execute_reply.started":"2025-05-03T19:24:47.5998Z","shell.execute_reply":"2025-05-03T19:24:47.619942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nfrom PIL import Image\nimport os\n\ngenai.configure(api_key=\"AIzaSyAyQ9vilRYQmaDVhAvtCUyU--HhaDjt9eo\")\n\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")\n\nimage_path = os.path.join(image_base_path, path)\nimage = Image.open(image_path)\n\n\n\nprompt = '''You are shown a product image along with its metadata. Use the metadata only for *contextual understanding*, not for answering. Your task is to generate a set of **4–6 diverse, image-answerable, and challenging questions**, where **each answer is a single word** (e.g., \"blue\", \"five\", \"yes\", \"sneakers\").\n\nThese questions must require **careful visual inspection**, **fine-grained detail recognition**, or **higher-level visual reasoning**, but must be **fully answerable using only the image itself**. The metadata is provided to help disambiguate what the image might contain — do not base your questions on metadata-only information.\n\nQuestion Requirements:\n- Must be **clearly grounded in visible evidence** in the image\n- Must be **unambiguous**, **non-subjective**, and **non-trivial**\n- Each answer must be a **single word** (e.g., noun, color, number, \"yes\"/\"no\")\n\nCoverage:\nInclude **at least one** question that requires:\n1. Small detail recognition \n2. Fine-grained categorization \n3. Challenging reasoning or fine-grained counting\n\nAlso consider:\n- Material/texture recognition\n- Color attribute specificity\n- Brand/logo identification\n- Shape or structure understanding\n- Functional yes/no questions\n\nDifficulty:\n- Mix easy, moderate, and hard — with emphasis on **moderate to hard**\n- Avoid obvious or metadata-derived questions\n- Avoid subjective or vague terms like \"modern\" or \"comfortable\"\n\nOutput Format:\nReturn only a list of dictionaries in this exact format:\n\n[\n    {\n        \"question\": \"What specific type of product is shown?\",\n        \"answer\": \"Backpack\"\n    },\n    {\n        \"question\": \"What material is most visible on the product?\",\n        \"answer\": \"Leather\"\n    },\n    {\n        \"question\": \"What color is the zipper?\",\n        \"answer\": \"Silver\"\n    },\n    {\n        \"question\": \"Is the logo present on the front side?\",\n        \"answer\": \"Yes\"\n    },\n    {\n        \"question\": \"How many pockets are visible?\",\n        \"answer\": \"Three\"\n    }\n]\n''' + f\"Image Metadata: {extract_metadata(val)}\"\n\nprint(\"Generating response\")\nresponse = model.generate_content([image, prompt])\ntext = response.text\n\nprint(text)\nprint(extract_json(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T19:25:01.21697Z","iopub.execute_input":"2025-05-03T19:25:01.217519Z","iopub.status.idle":"2025-05-03T19:25:06.43792Z","shell.execute_reply.started":"2025-05-03T19:25:01.217447Z","shell.execute_reply":"2025-05-03T19:25:06.436967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport pandas as pd\nfrom PIL import Image\nimport google.generativeai as genai\nfrom tqdm import tqdm\n\n# === CONFIGURATION ===\nAPI_KEY = \"AIzaSyDW7iUb6rnEZqrzA8hOa1qBT4wSrJsl9GA\"\nLISTINGS_JSON_PATH = \"/kaggle/input/product-listings/product_listings.json\"\nOUTPUT_JSON = \"vqa_results_24400_to_25800.json\"\n\nSTART_INDEX = 24400\nEND_INDEX = 25800\n\n\nSLEEP_TIME = 3.5  # in seconds\n\n# === Load listings JSON ===\nwith open(LISTINGS_JSON_PATH, \"r\") as f:\n    data = json.load(f)\n\n\nkeys = list(data.keys())\ndata_subset = {k: data[k] for k in keys[START_INDEX:END_INDEX]}\n\n# === Setup Gemini ===\ngenai.configure(api_key=API_KEY)\nmodel = genai.GenerativeModel(\"gemini-2.0-flash\")\n\n\n\nall_results = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T23:55:39.637021Z","iopub.execute_input":"2025-05-03T23:55:39.638371Z","iopub.status.idle":"2025-05-03T23:55:50.940756Z","shell.execute_reply.started":"2025-05-03T23:55:39.6383Z","shell.execute_reply":"2025-05-03T23:55:50.939902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# === Iterate through listings ===\ncount = 0\nfor key, val in tqdm(data_subset.items()):\n\n    try:\n        image_id = val.get(\"main_image_id\")\n        if not image_id:\n            continue\n\n        # Retrieve image path\n        path = metadata_df.at[image_id, \"path\"]\n        image_path = os.path.join(image_base_path, path)\n        if not os.path.exists(image_path):\n            continue\n\n        # Open image\n        image = Image.open(image_path)\n\n        # Compose prompt\n        prompt = '''You are shown a product image along with its metadata. Use the metadata only for *contextual understanding*, not for answering. Your task is to generate a set of **4–6 diverse, image-answerable, and challenging questions**, where **each answer is a single word** (e.g., \"blue\", \"five\", \"yes\", \"sneakers\").\n\nThese questions must require **careful visual inspection**, **fine-grained detail recognition**, or **higher-level visual reasoning**, but must be **fully answerable using only the image itself**. The metadata is provided to help disambiguate what the image might contain — do not base your questions on metadata-only information.\n\nQuestion Requirements:\n- Must be **clearly grounded in visible evidence** in the image\n- Must be **unambiguous**, **non-subjective**, and **non-trivial**\n- Each answer must be a **single word**\n\nCoverage:\nInclude **at least one** question that requires:\n1. Small detail recognition \n2. Fine-grained categorization \n3. Challenging reasoning or fine-grained counting\n\nAlso consider:\n- Material/texture recognition\n- Color attribute specificity\n- Brand/logo identification\n- Shape or structure understanding\n- Functional yes/no questions\n- Any other visible aspects of the image\n\nDifficulty:\n- Mix easy, moderate, and hard — with emphasis on **moderate to hard**\n- Avoid obvious or metadata-derived questions\n- Avoid subjective or vague terms like \"modern\" or \"comfortable\"\n\nOutput Format:\nReturn only a list of dictionaries in this exact format:\n\n[\n    {\n        \"question\": \"What specific type of product is shown?\",\n        \"answer\": \"Backpack\"\n    },\n    {\n        \"question\": \"What material is most visible on the product?\",\n        \"answer\": \"Leather\"\n    },\n    {\n        \"question\": \"What color is the zipper?\",\n        \"answer\": \"Silver\"\n    },\n    {\n        \"question\": \"Is the logo present on the front side?\",\n        \"answer\": \"Yes\"\n    },\n    {\n        \"question\": \"How many pockets are visible?\",\n        \"answer\": \"Three\"\n    }\n]\n''' + f\"\\nImage Metadata: {extract_metadata(val)}\"\n\n        print(f\"Generating response for {key}...\")\n        response = model.generate_content([image, prompt])\n        qa_data = response.text.strip()\n        final_qa = extract_json(qa_data)\n\n        all_results[key] = {\n            \"image_id\": image_id,\n            \"qa_data\": final_qa\n        }\n\n        # Save intermediate result\n        with open(OUTPUT_JSON, \"w\") as f:\n            json.dump(all_results, f, indent=2)\n\n        count += 1\n        time.sleep(SLEEP_TIME)\n\n    except Exception as e:\n        print(f\"[ERROR] Failed for {key}: {e}\")\n        time.sleep(SLEEP_TIME * 2)  # longer pause after error\n        continue\n\nprint(f\"\\n Completed {count} listings. Results saved to {OUTPUT_JSON}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T23:55:57.31198Z","iopub.execute_input":"2025-05-03T23:55:57.31265Z","iopub.status.idle":"2025-05-04T01:52:38.310941Z","shell.execute_reply.started":"2025-05-03T23:55:57.312617Z","shell.execute_reply":"2025-05-04T01:52:38.309396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering, BartTokenizer, BartForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n# import evaluate\n# from bert_score import score as bert_score  # Uncomment if installed\nimport csv\nfrom pathlib import Path\n\n# Prepare output CSV\noutput_path = Path(\"vqa_test_predictions.csv\")\n\n# Config\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntotal_images = 1000  # total available\neval_fraction = 0.3   # 20% of data for baseline evaluation\n\n# Load BLIP\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n\n# Load dataset and sample 20%\ndf = pd.read_csv(\"/kaggle/input/combined-vqa/flattened_vqa_with_metadata.csv\")\ndf_sampled = df.sample(frac=eval_fraction, random_state=42).copy()\n\n# Train-test split (e.g., 80-20)\ntrain_df, test_df = train_test_split(df_sampled, test_size=0.2, random_state=42)\n\n# Only evaluate on test set\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['image_path']\n        question = row['question']\n        ground_truth = row['answer']\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n        output = model.generate(**inputs)\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        predictions.append(answer)\n    except Exception as e:\n        print(f\"Failed on {row['image_path']}: {e}\")\n        predictions.append(\"\")\n        \n    with open(output_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        writer.writerow([image_path, question, ground_truth, answer])\n    print(\"predictions saved to csv\")\n# Save and evaluate\n# test_df = test_df.copy()\n# test_df['predicted_answer'] = predictions\n# test_df['correct'] = test_df['predicted_answer'].str.strip().str.lower() == test_df['answer'].str.strip().str.lower()\n# test_df.to_csv(\"vqa_test_predictions.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:34:06.796047Z","iopub.execute_input":"2025-05-12T16:34:06.796260Z","iopub.status.idle":"2025-05-12T16:50:29.338116Z","shell.execute_reply.started":"2025-05-12T16:34:06.796241Z","shell.execute_reply":"2025-05-12T16:50:29.337353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:55:09.178717Z","iopub.execute_input":"2025-05-12T16:55:09.179365Z","iopub.status.idle":"2025-05-12T16:55:09.207278Z","shell.execute_reply.started":"2025-05-12T16:55:09.179337Z","shell.execute_reply":"2025-05-12T16:55:09.206671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bert-score rouge-score nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T06:29:09.636841Z","iopub.execute_input":"2025-05-13T06:29:09.637164Z","iopub.status.idle":"2025-05-13T06:31:00.362385Z","shell.execute_reply.started":"2025-05-13T06:29:09.637135Z","shell.execute_reply":"2025-05-13T06:31:00.360640Z"}},"outputs":[{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=bc532c7980ad41296fa1cb555cfefb016e514d266ee46e6153162f420e04561c\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rouge-score, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel, PeftConfig\nimport torch\n\n# Load processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load base model\nbase_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/16-16-full-no-dense/transformers/default/1\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:49:51.667573Z","iopub.execute_input":"2025-05-13T18:49:51.668338Z","iopub.status.idle":"2025-05-13T18:49:54.226147Z","shell.execute_reply.started":"2025-05-13T18:49:51.668307Z","shell.execute_reply":"2025-05-13T18:49:54.225286Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BlipForQuestionAnswering(\n      (vision_model): BlipVisionModel(\n        (embeddings): BlipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (encoder): BlipEncoder(\n          (layers): ModuleList(\n            (0-11): 12 x BlipEncoderLayer(\n              (self_attn): BlipAttention(\n                (dropout): Dropout(p=0.0, inplace=False)\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2304, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (projection): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (mlp): BlipMLP(\n                (activation_fn): GELUActivation()\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3072, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3072, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (text_encoder): BlipTextModel(\n        (embeddings): BlipTextEmbeddings(\n          (word_embeddings): Embedding(30524, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (encoder): BlipTextEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BlipTextLayer(\n              (attention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (crossattention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (intermediate): BlipTextIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BlipTextOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (text_decoder): BlipTextLMHeadModel(\n        (bert): BlipTextModel(\n          (embeddings): BlipTextEmbeddings(\n            (word_embeddings): Embedding(30524, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (encoder): BlipTextEncoder(\n            (layer): ModuleList(\n              (0-11): 12 x BlipTextLayer(\n                (attention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (crossattention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (intermediate): BlipTextIntermediate(\n                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BlipTextOutput(\n                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (cls): BlipTextOnlyMLMHead(\n          (predictions): BlipTextLMPredictionHead(\n            (transform): BlipTextPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (transform_act_fn): GELUActivation()\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (decoder): Linear(in_features=768, out_features=30524, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Load your test CSV (replace with actual path if needed)\ntest_df = pd.read_csv(\"/kaggle/input/combined-vqa/flattened_vqa_with_metadata.csv\")  # or whatever test set you want\ntest_df = test_df.sample(n=30000, random_state=42).copy()  # Adjust sample size if needed\n\n# Prepare predictions\npredictions = []\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running Inference\"):\n    try:\n        image_path = row['image_path']\n        question = str(row['question'])\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Preprocess\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n        # Generate answer\n        with torch.no_grad():\n            output = model.generate(**inputs)\n\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        predictions.append(answer)\n    except Exception as e:\n        print(f\"Error with {image_path}: {e}\")\n        predictions.append(\"\")\n\n# Save predictions to CSV\ntest_df['predicted_answer'] = predictions\ntest_df.to_csv(\"blip_16-16-full-no-dense-epoch2-finetuned_predictions.csv\", index=False)\nprint(\"Predictions saved to blip_8-16-qkv-finetuned_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:55:16.507353Z","iopub.execute_input":"2025-05-13T18:55:16.507919Z","iopub.status.idle":"2025-05-13T19:08:14.445609Z","shell.execute_reply.started":"2025-05-13T18:55:16.507885Z","shell.execute_reply":"2025-05-13T19:08:14.443760Z"}},"outputs":[{"name":"stderr","text":"Running Inference:   2%|▏         | 513/30000 [12:57<12:24:48,  1.52s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3114904137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Generate answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_base_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_prompt_tuning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, pixel_values, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[0;32m-> 1420\u001b[0;31m         vision_outputs = self.vision_model(\n\u001b[0m\u001b[1;32m   1421\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    672\u001b[0m                 )\n\u001b[1;32m    673\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                 layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    675\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         hidden_states, attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m def softmax(\n\u001b[0m\u001b[1;32m   2104\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"!pip install bert-score rouge-score nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:43:58.719310Z","iopub.execute_input":"2025-05-13T18:43:58.719624Z","iopub.status.idle":"2025-05-13T18:45:45.656641Z","shell.execute_reply.started":"2025-05-13T18:43:58.719600Z","shell.execute_reply":"2025-05-13T18:45:45.655231Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=05c3b19634908a44e2b2dbe71f491df93d144c23d4197faf90de6c2de30b246e\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rouge-score, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# from bart_score import BARTScorer\nfrom tqdm import tqdm\n\ntqdm.pandas()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:45:52.952846Z","iopub.execute_input":"2025-05-13T18:45:52.953278Z","iopub.status.idle":"2025-05-13T18:45:53.746608Z","shell.execute_reply.started":"2025-05-13T18:45:52.953241Z","shell.execute_reply":"2025-05-13T18:45:53.745928Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load CSV\ndf = pd.read_csv(\"/kaggle/working/blip_16-16-full-no-dense-epoch2-finetuned_predictions.csv\")  # Replace with your CSV file path\n\n# Clean text (optional but recommended)\ndef normalize_text(s):\n    return str(s).strip().lower()\n\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Accuracy\naccuracy = (df['answer'] == df['predicted_answer']).mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# F1 Score (Token-level macro F1)\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn), axis=1)\navg_bleu = df['bleu'].mean()\n\n# # BARTScore (optional; needs large model and GPU)\n# bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n# df['bartscore'] = df.progress_apply(lambda row: bart_scorer.score([row['answer']], [row['ground_truth']])[0], axis=1)\n# avg_bartscore = df['bartscore'].mean()\n\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")\n# print(f\"BARTScore     : {avg_bartscore:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:53:11.931731Z","iopub.execute_input":"2025-05-13T18:53:11.932055Z","iopub.status.idle":"2025-05-13T18:53:14.578943Z","shell.execute_reply.started":"2025-05-13T18:53:11.932025Z","shell.execute_reply":"2025-05-13T18:53:14.577550Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ec2fe8f72dc4716a107c9f55622f97e"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef5cfe062ebe4e7d8e2809eccdfbd9c5"}},"metadata":{}},{"name":"stdout","text":"done in 2.00 seconds, 49.98 sentences/sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:00<00:00, 14258.10it/s]\n100%|██████████| 100/100 [00:00<00:00, 60990.32it/s]\n100%|██████████| 100/100 [00:00<00:00, 14868.15it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy      : 0.6100\nBERTScore F1  : 0.9836\nROUGE-L       : 0.6200\nToken F1      : 0.6100\nBLEU          : 0.1085\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8}]}
