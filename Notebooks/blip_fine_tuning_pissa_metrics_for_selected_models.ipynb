{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5146007,"sourceType":"datasetVersion","datasetId":2989778},{"sourceId":11472097,"sourceType":"datasetVersion","datasetId":7189537},{"sourceId":11766806,"sourceType":"datasetVersion","datasetId":7387101},{"sourceId":11772056,"sourceType":"datasetVersion","datasetId":7390719},{"sourceId":389534,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":320895,"modelId":341493},{"sourceId":390965,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":321968,"modelId":342589}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:56:50.900741Z","iopub.execute_input":"2025-05-12T16:56:50.901016Z","iopub.status.idle":"2025-05-12T16:58:01.16458Z","shell.execute_reply.started":"2025-05-12T16:56:50.900991Z","shell.execute_reply":"2025-05-12T16:58:01.163868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom transformers import BlipProcessor, BlipForQuestionAnswering, BartTokenizer, BartForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n# import evaluate\n# from bert_score import score as bert_score\n\nfrom transformers import (\n    BlipForQuestionAnswering,\n    BlipProcessor\n)\n\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:02:02.607112Z","iopub.execute_input":"2025-05-13T10:02:02.607357Z","iopub.status.idle":"2025-05-13T10:02:39.277545Z","shell.execute_reply.started":"2025-05-13T10:02:02.607331Z","shell.execute_reply":"2025-05-13T10:02:39.276921Z"}},"outputs":[{"name":"stderr","text":"2025-05-13 10:02:18.679234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747130538.857470      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747130538.911222      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"962195454650465cb28d3e8567107020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a110a917481c480d8f76bca7eec026da"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"616c979bf00c4a50800e674b3bdef1c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b169d893f74cefb69569eddd8a7dcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21442bbf525a4df39aed487d5c0f041c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfff8a21b988405ba1e5f11b3d32af3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3229dad79bd9456cbf651e1780b2c2f7"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/combined-vqa/flattened_vqa_with_metadata.csv')\ndf = df.sample(frac=0.4, random_state=42).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:34:35.464368Z","iopub.execute_input":"2025-05-13T13:34:35.465121Z","iopub.status.idle":"2025-05-13T13:34:35.774145Z","shell.execute_reply.started":"2025-05-13T13:34:35.465092Z","shell.execute_reply":"2025-05-13T13:34:35.773227Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# import os\n# import io\n# import torch\n# import warnings\n# from PIL import Image\n# import contextlib\n# import pandas as pd\n\n\n# # Assuming processor and model are already initialized and on CUDA\n# # image_base_path should be the root of your image folder\n# image_base_path = \"/kaggle/input/abo-small/small\"\n\n# skipped = 0\n# skip_logs = []\n\n# for idx, row in df.iterrows():\n#     question = row[\"question\"]\n#     gt_answer = str(row[\"answer\"]).strip().lower()\n#     image_rel_path = row[\"image_path\"]\n\n#     full_path = image_rel_path  # Already contains full relative path\n#     if not os.path.exists(full_path):\n#         skip_logs.append(f\"[SKIP] Image not found: {full_path}\")\n#         skipped += 1\n#         continue\n\n#     try:\n#         image = Image.open(full_path).convert(\"RGB\")\n#     except Exception as e:\n#         skip_logs.append(f\"[SKIP] Failed to load image: {full_path} | Error: {e}\")\n#         skipped += 1\n#         continue\n\n#     with warnings.catch_warnings():\n#         warnings.simplefilter(\"ignore\")\n#         with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n#             inputs = processor(images=image, text=question, return_tensors=\"pt\")\n#             output = model.generate(**inputs, max_new_tokens=10)\n#             pred_answer = processor.decode(output[0], skip_special_tokens=True).strip().lower()\n\n#     print(f\"Q: {question}\\nPredicted: {pred_answer} | Ground Truth: {gt_answer}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:45:26.890893Z","iopub.execute_input":"2025-05-12T16:45:26.891148Z","iopub.status.idle":"2025-05-12T16:45:27.256737Z","shell.execute_reply.started":"2025-05-12T16:45:26.891121Z","shell.execute_reply":"2025-05-12T16:45:27.255983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:45:27.257541Z","iopub.execute_input":"2025-05-12T16:45:27.257817Z","iopub.status.idle":"2025-05-12T16:45:33.709004Z","shell.execute_reply.started":"2025-05-12T16:45:27.257772Z","shell.execute_reply":"2025-05-12T16:45:33.708285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:45:33.710002Z","iopub.execute_input":"2025-05-12T16:45:33.710298Z","iopub.status.idle":"2025-05-12T16:45:37.254051Z","shell.execute_reply.started":"2025-05-12T16:45:33.710255Z","shell.execute_reply":"2025-05-12T16:45:37.253095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from peft import get_peft_model, LoraConfig, TaskType\n\n\n\n\n# # LoRA configuration\n# lora_config = LoraConfig(\n#     r=4,\n#     lora_alpha=16,\n#     target_modules=[\"q\", \"v\"],  # 'q' and 'v' are common; confirm exact target modules if needed\n#     lora_dropout=0.1,\n#     bias=\"none\",\n#     task_type=TaskType.SEQ_2_SEQ_LM\n# )\n\n# # Apply LoRA\n# model = get_peft_model(model, lora_config)\n\n# # Print number of trainable parameters\n# def print_trainable_params(model):\n#     trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     total = sum(p.numel() for p in model.parameters())\n#     print(f\"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n\n# print_trainable_params(model)\n\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n\nlora_config = LoraConfig(\n    # use_rslora = True,\n    init_lora_weights = \"pissa\",\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\", \"projection\", \"fc1\", \"fc2\", \"key\", \"qkv\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:35:21.459306Z","iopub.execute_input":"2025-05-13T13:35:21.459875Z","iopub.status.idle":"2025-05-13T13:35:40.310570Z","shell.execute_reply.started":"2025-05-13T13:35:21.459851Z","shell.execute_reply":"2025-05-13T13:35:40.309647Z"}},"outputs":[{"name":"stdout","text":"trainable params: 5,898,240 || all params: 390,570,812 || trainable%: 1.5102\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport requests\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport pickle\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Dataset class modification for CSV format\nclass CSVVQADataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, processor):\n        self.data = dataframe\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        image = Image.open(item['image_path']).convert(\"RGB\")\n        text = item['question']\n        answer = item['answer']\n        if isinstance(answer, list):\n            answer = \" \".join(map(str, answer))  # convert list items to str if needed\n        elif not isinstance(answer, str):\n            answer = str(answer)\n        \n        if isinstance(text, list):\n            text = \" \".join(map(str, text))  # convert list items to str if needed\n        elif not isinstance(text, str):\n            text = str(text)\n            \n        encoding = self.processor(\n            image, \n            text, \n            max_length = 128,\n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n        \n        labels = self.processor.tokenizer(\n            answer, \n            max_length= 128, \n            padding=\"max_length\", \n            return_tensors='pt'\n        )\n        \n        encoding[\"labels\"] = labels[\"input_ids\"].squeeze()\n        for k,v in encoding.items():  \n            encoding[k] = v.squeeze()\n        return encoding\n\n# Load and split data\ntrain_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Create datasets\ntrain_dataset = CSVVQADataset(train_df, processor)\nvalid_dataset = CSVVQADataset(valid_df, processor)\n\n# Create dataloaders\nbatch_size = 3\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True,  # Enable shuffling\n    pin_memory=True\n)\nvalid_dataloader = DataLoader(\n    valid_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    pin_memory=True\n)\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nscaler = torch.cuda.amp.GradScaler()\n\n# Training Loop\nmin_eval_loss = float(\"inf\")\ntracking_information = []\npatience = 3\nearly_stopping_hook = 0\n\nfor epoch in range(2):\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\"):\n        for k in batch:\n            batch[k] = batch[k].to(device)\n        with torch.cuda.amp.autocast():\n            outputs = model(**batch)\n            loss = outputs.loss\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        train_loss += loss.item()\n    \n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(valid_dataloader, desc=f\"Epoch {epoch+1} Validation\"):\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(**batch)\n                loss = outputs.loss\n            eval_loss += loss.item()\n    \n    train_loss /= len(train_dataloader)\n    eval_loss /= len(valid_dataloader)\n    tracking_information.append((train_loss, eval_loss))\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Eval Loss = {eval_loss:.4f}\")\n\n    if eval_loss < min_eval_loss:\n        min_eval_loss = eval_loss\n        model.save_pretrained(\"Model/blip-lora-saved\")\n        model.save_pretrained(\"/kaggle/working/blip-lora-saved\")\n        print(\"✅ Model saved.\")\n        early_stopping_hook = 0\n    else:\n        early_stopping_hook += 1\n        if early_stopping_hook >= patience:\n            print(\"⏹️ Early stopping.\")\n            break\n\npickle.dump(tracking_information, open(\"tracking_info.pkl\", \"wb\"))\nprint(\"🎉 Finetuning complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:35:54.120464Z","iopub.execute_input":"2025-05-13T13:35:54.121229Z","iopub.status.idle":"2025-05-13T17:55:54.608297Z","shell.execute_reply.started":"2025-05-13T13:35:54.121201Z","shell.execute_reply":"2025-05-13T17:55:54.607494Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/266748961.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\nEpoch 1 Training:   0%|          | 0/16979 [00:00<?, ?it/s]/tmp/ipykernel_31/266748961.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1 Training: 100%|██████████| 16979/16979 [1:57:21<00:00,  2.41it/s] \nEpoch 1 Validation:   0%|          | 0/4245 [00:00<?, ?it/s]/tmp/ipykernel_31/266748961.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1 Validation: 100%|██████████| 4245/4245 [12:33<00:00,  5.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 8.5978, Eval Loss = 8.5326\n✅ Model saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|██████████| 16979/16979 [1:57:36<00:00,  2.41it/s] \nEpoch 2 Validation: 100%|██████████| 4245/4245 [12:28<00:00,  5.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 8.5295, Eval Loss = 8.5281\n✅ Model saved.\n🎉 Finetuning complete!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel, PeftConfig\nimport torch\n\n# Load processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load base model\nbase_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"Model/blip-lora-saved\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:04:24.779297Z","iopub.execute_input":"2025-05-13T01:04:24.780066Z","iopub.status.idle":"2025-05-13T01:04:27.862836Z","shell.execute_reply.started":"2025-05-13T01:04:24.780031Z","shell.execute_reply":"2025-05-13T01:04:27.862206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your test CSV (replace with actual path if needed)\ntest_df = pd.read_csv(\"/kaggle/input/combined-vqa/flattened_vqa_with_metadata.csv\")  # or whatever test set you want\ntest_df = test_df.sample(n=100, random_state=42).copy()  # 🔁 Adjust sample size if needed\n\n# Prepare predictions\npredictions = []\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running Inference\"):\n    try:\n        image_path = row['image_path']\n        question = str(row['question'])\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Preprocess\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n        # Generate answer\n        with torch.no_grad():\n            output = model.generate(**inputs)\n\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        predictions.append(answer)\n    except Exception as e:\n        print(f\"❌ Error with {image_path}: {e}\")\n        predictions.append(\"\")\n\n# Save predictions to CSV\ntest_df['predicted_answer'] = predictions\ntest_df.to_csv(\"blip_finetuned_predictions.csv\", index=False)\nprint(\"✅ Predictions saved to blip_finetuned_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:05:23.56329Z","iopub.execute_input":"2025-05-13T01:05:23.563896Z","iopub.status.idle":"2025-05-13T01:05:34.258244Z","shell.execute_reply.started":"2025-05-13T01:05:23.563851Z","shell.execute_reply":"2025-05-13T01:05:34.257599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport io\nimport torch\nimport warnings\nfrom PIL import Image\nimport contextlib\nimport pandas as pd\n\n\n# Limit to first 5 rows\ndf = df.head(5)\n\n# Assuming processor and model are already initialized and on CUDA\n# image_base_path should be the root of your image folder\nimage_base_path = \"/kaggle/input/abo-small/small\"\n\nskipped = 0\nskip_logs = []\n\nfor idx, row in df.iterrows():\n    question = row[\"question\"]\n    gt_answer = str(row[\"answer\"]).strip().lower()\n    image_rel_path = row[\"image_path\"]\n\n    full_path = image_rel_path  # Already contains full relative path\n    if not os.path.exists(full_path):\n        skip_logs.append(f\"[SKIP] Image not found: {full_path}\")\n        skipped += 1\n        continue\n\n    try:\n        image = Image.open(full_path).convert(\"RGB\")\n    except Exception as e:\n        skip_logs.append(f\"[SKIP] Failed to load image: {full_path} | Error: {e}\")\n        skipped += 1\n        continue\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n            inputs = processor(images=image, text=question, return_tensors=\"pt\")\n            output = model.generate(**inputs, max_new_tokens=10)\n            pred_answer = processor.decode(output[0], skip_special_tokens=True).strip().lower()\n\n    print(f\"Q: {question}\\nPredicted: {pred_answer} | Ground Truth: {gt_answer}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:50:25.007192Z","iopub.status.idle":"2025-05-12T16:50:25.007515Z","shell.execute_reply.started":"2025-05-12T16:50:25.007366Z","shell.execute_reply":"2025-05-12T16:50:25.007383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel, PeftConfig\nimport torch\n\n# Load processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load base model\nbase_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/16-16-full-no-dense/transformers/default/1\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:08:14.943293Z","iopub.execute_input":"2025-05-13T19:08:14.943493Z","iopub.status.idle":"2025-05-13T19:08:52.465096Z","shell.execute_reply.started":"2025-05-13T19:08:14.943476Z","shell.execute_reply":"2025-05-13T19:08:52.464318Z"}},"outputs":[{"name":"stderr","text":"2025-05-13 19:08:29.808842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747163310.035460      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747163310.099833      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c044af6a57b487a9dcb123ce013c102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40440222ff3f4bcaaa5a95a3cd0c0b0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4618375e094fb989d4aa2fd5913470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc7283c6797a4422ab7522a9ad4846cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e51ef3103a84465960a8e9f6eebed5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b2df3908be4511ba736b740872b5fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9063b87a0cc459d9f241aafe690cb3b"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BlipForQuestionAnswering(\n      (vision_model): BlipVisionModel(\n        (embeddings): BlipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (encoder): BlipEncoder(\n          (layers): ModuleList(\n            (0-11): 12 x BlipEncoderLayer(\n              (self_attn): BlipAttention(\n                (dropout): Dropout(p=0.0, inplace=False)\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2304, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (projection): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (mlp): BlipMLP(\n                (activation_fn): GELUActivation()\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3072, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3072, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (text_encoder): BlipTextModel(\n        (embeddings): BlipTextEmbeddings(\n          (word_embeddings): Embedding(30524, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (encoder): BlipTextEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BlipTextLayer(\n              (attention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (crossattention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (intermediate): BlipTextIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BlipTextOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (text_decoder): BlipTextLMHeadModel(\n        (bert): BlipTextModel(\n          (embeddings): BlipTextEmbeddings(\n            (word_embeddings): Embedding(30524, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (encoder): BlipTextEncoder(\n            (layer): ModuleList(\n              (0-11): 12 x BlipTextLayer(\n                (attention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (crossattention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (intermediate): BlipTextIntermediate(\n                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BlipTextOutput(\n                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (cls): BlipTextOnlyMLMHead(\n          (predictions): BlipTextLMPredictionHead(\n            (transform): BlipTextPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (transform_act_fn): GELUActivation()\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (decoder): Linear(in_features=768, out_features=30524, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Load your test CSV (replace with actual path if needed)\ntest_df = pd.read_csv(\"/kaggle/input/combined-vqa/flattened_vqa_with_metadata.csv\")  # or whatever test set you want\ntest_df = test_df.sample(n=30000, random_state=42).copy()  # 🔁 Adjust sample size if needed\n\n# Prepare predictions\npredictions = []\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running Inference\"):\n    try:\n        image_path = row['image_path']\n        question = str(row['question'])\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Preprocess\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n        # Generate answer\n        with torch.no_grad():\n            output = model.generate(**inputs)\n\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        predictions.append(answer)\n    except Exception as e:\n        print(f\"❌ Error with {image_path}: {e}\")\n        predictions.append(\"\")\n\n# Save predictions to CSV\ntest_df['predicted_answer'] = predictions\ntest_df.to_csv(\"blip_16-16-full-no-dense-epoch2-finetuned_predictions.csv\", index=False)\nprint(\"✅ Predictions saved to blip_8-16-qkv-finetuned_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:09:03.428083Z","iopub.execute_input":"2025-05-13T19:09:03.428582Z","iopub.status.idle":"2025-05-13T20:10:14.422194Z","shell.execute_reply.started":"2025-05-13T19:09:03.428557Z","shell.execute_reply":"2025-05-13T20:10:14.421614Z"}},"outputs":[{"name":"stderr","text":"Running Inference: 100%|██████████| 30000/30000 [1:01:10<00:00,  8.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Predictions saved to blip_8-16-qkv-finetuned_predictions.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install bert-score rouge-score nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:11:04.432723Z","iopub.execute_input":"2025-05-13T20:11:04.432983Z","iopub.status.idle":"2025-05-13T20:12:22.612630Z","shell.execute_reply.started":"2025-05-13T20:11:04.432963Z","shell.execute_reply":"2025-05-13T20:12:22.611861Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=57fd8c396358b2b40fa16c73bf1a5e10d48fe5850ff10d46104ed17ea8862e50\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rouge-score, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# from bart_score import BARTScorer\nfrom tqdm import tqdm\n\ntqdm.pandas()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:12:24.985563Z","iopub.execute_input":"2025-05-13T20:12:24.985877Z","iopub.status.idle":"2025-05-13T20:12:25.773209Z","shell.execute_reply.started":"2025-05-13T20:12:24.985850Z","shell.execute_reply":"2025-05-13T20:12:25.772170Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load CSV\ndf = pd.read_csv(\"/kaggle/working/blip_16-16-full-no-dense-epoch2-finetuned_predictions.csv\")  # Replace with your CSV file path\n\n# Clean text (optional but recommended)\ndef normalize_text(s):\n    return str(s).strip().lower()\n\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Accuracy\naccuracy = (df['answer'] == df['predicted_answer']).mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# F1 Score (Token-level macro F1)\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn), axis=1)\navg_bleu = df['bleu'].mean()\n\n# # BARTScore (optional; needs large model and GPU)\n# bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n# df['bartscore'] = df.progress_apply(lambda row: bart_scorer.score([row['answer']], [row['ground_truth']])[0], axis=1)\n# avg_bartscore = df['bartscore'].mean()\n\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")\n# print(f\"BARTScore     : {avg_bartscore:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T20:12:29.877427Z","iopub.execute_input":"2025-05-13T20:12:29.877724Z","iopub.status.idle":"2025-05-13T20:12:53.541616Z","shell.execute_reply.started":"2025-05-13T20:12:29.877669Z","shell.execute_reply":"2025-05-13T20:12:53.540813Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f9eba254474f63ae6e5f6c492166ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d47b073b5814358a2730faa807a0693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241fec6174674ff195233c5b1e4a22e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c1396a800614059ad903f0a6399feed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52bf3a3616d847da875a53798e34aafb"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd74f0a2158d4f6c857d19a078a58e35"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/42 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f46fdad816724237a4c9bbb031db7805"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/469 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb1ff610cfc4e60bfdf11de829ac758"}},"metadata":{}},{"name":"stdout","text":"done in 7.43 seconds, 4036.15 sentences/sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30000/30000 [00:01<00:00, 18680.36it/s]\n100%|██████████| 30000/30000 [00:00<00:00, 141344.98it/s]\n100%|██████████| 30000/30000 [00:01<00:00, 20592.26it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy      : 0.6155\nBERTScore F1  : 0.9842\nROUGE-L       : 0.6247\nToken F1      : 0.6155\nBLEU          : 0.1095\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel, PeftConfig\nimport torch\n\n# Load processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load base model\nbase_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/blip-16-16-no-dense/transformers/default/1\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:46:47.725296Z","iopub.execute_input":"2025-05-13T22:46:47.725931Z","iopub.status.idle":"2025-05-13T22:46:51.021084Z","shell.execute_reply.started":"2025-05-13T22:46:47.725903Z","shell.execute_reply":"2025-05-13T22:46:51.020378Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BlipForQuestionAnswering(\n      (vision_model): BlipVisionModel(\n        (embeddings): BlipVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (encoder): BlipEncoder(\n          (layers): ModuleList(\n            (0-11): 12 x BlipEncoderLayer(\n              (self_attn): BlipAttention(\n                (dropout): Dropout(p=0.0, inplace=False)\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=2304, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (projection): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (mlp): BlipMLP(\n                (activation_fn): GELUActivation()\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=768, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3072, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3072, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=768, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (text_encoder): BlipTextModel(\n        (embeddings): BlipTextEmbeddings(\n          (word_embeddings): Embedding(30524, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (encoder): BlipTextEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BlipTextLayer(\n              (attention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (crossattention): BlipTextAttention(\n                (self): BlipTextSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): BlipTextSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (intermediate): BlipTextIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BlipTextOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (text_decoder): BlipTextLMHeadModel(\n        (bert): BlipTextModel(\n          (embeddings): BlipTextEmbeddings(\n            (word_embeddings): Embedding(30524, 768, padding_idx=0)\n            (position_embeddings): Embedding(512, 768)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (encoder): BlipTextEncoder(\n            (layer): ModuleList(\n              (0-11): 12 x BlipTextLayer(\n                (attention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (crossattention): BlipTextAttention(\n                  (self): BlipTextSelfAttention(\n                    (query): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (key): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (value): lora.Linear(\n                      (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.1, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=768, out_features=16, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=16, out_features=768, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (output): BlipTextSelfOutput(\n                    (dense): Linear(in_features=768, out_features=768, bias=True)\n                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                    (dropout): Dropout(p=0.0, inplace=False)\n                  )\n                )\n                (intermediate): BlipTextIntermediate(\n                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n                  (intermediate_act_fn): GELUActivation()\n                )\n                (output): BlipTextOutput(\n                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n            )\n          )\n        )\n        (cls): BlipTextOnlyMLMHead(\n          (predictions): BlipTextLMPredictionHead(\n            (transform): BlipTextPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (transform_act_fn): GELUActivation()\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            )\n            (decoder): Linear(in_features=768, out_features=30524, bias=True)\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Load your test CSV (replace with actual path if needed)\n# test_df = pd.read_csv(\"/kaggle/input/combined-vqa/flattened_vqa_with_metadata.csv\")  # or whatever test set you want\n# test_df = test_df.sample(n=30000, random_state=42).copy()  # 🔁 Adjust sample size if needed\n\nimport pandas as pd\n\n# Path to your .txt file\ntxt_path = \"/kaggle/input/d/henrychibueze/vqa-dataset/vaq2.0.TestImages.txt\"\n\n# Read the file\nwith open(txt_path, \"r\") as f:\n    lines = f.readlines()\n\n# Parse lines\ndata = []\nfor line in lines:\n    try:\n        image_id, rest = line.strip().split(\"\\t\")\n        question, answer = rest.rsplit(\"?\", 1)\n        question = question.strip() + \"?\"\n        answer = answer.strip()\n        image_path = f\"/kaggle/input/d/henrychibueze/vqa-dataset/val2014-resised/{image_id.split('#')[0]}\"  # update this base path\n        data.append({\n            \"image_path\": image_path,\n            \"question\": question,\n            \"ground_truth_answer\": answer\n        })\n    except Exception as e:\n        print(f\"Error parsing line: {line} | {e}\")\n\n# Create DataFrame\ntest_df = pd.DataFrame(data)\ntest_df = test_df.sample(n=1000, random_state=42).copy()  # 🔁 Adjust sample size if needed\n\n# Prepare predictions\npredictions = []\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running Inference\"):\n    try:\n        image_path = row['image_path']\n        question = str(row['question'])\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Preprocess\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n\n        # Generate answer\n        with torch.no_grad():\n            output = model.generate(**inputs)\n\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        predictions.append(answer)\n    except Exception as e:\n        print(f\"❌ Error with {image_path}: {e}\")\n        predictions.append(\"\")\n\n# Save predictions to CSV\ntest_df['predicted_answer'] = predictions\ntest_df.to_csv(\"blip-vqa_16-16-no-dense-finetuned_predictions.csv\", index=False)\nprint(\"✅ Predictions saved to blip_8-16-qkv-finetuned_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:46:56.937531Z","iopub.execute_input":"2025-05-13T22:46:56.938069Z","iopub.status.idle":"2025-05-13T22:48:50.552405Z","shell.execute_reply.started":"2025-05-13T22:46:56.938047Z","shell.execute_reply":"2025-05-13T22:48:50.551625Z"}},"outputs":[{"name":"stderr","text":"Running Inference: 100%|██████████| 1000/1000 [01:53<00:00,  8.80it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Predictions saved to blip_8-16-qkv-finetuned_predictions.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install bert-score rouge-score nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:44:44.786987Z","iopub.execute_input":"2025-05-13T22:44:44.787290Z","iopub.status.idle":"2025-05-13T22:45:54.336765Z","shell.execute_reply.started":"2025-05-13T22:44:44.787269Z","shell.execute_reply":"2025-05-13T22:45:54.335822Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=1dddd3eb2f26efafde4782bfb74b0723724287036991098a1c3111609cb37f90\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rouge-score, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# from bart_score import BARTScorer\nfrom tqdm import tqdm\n\ntqdm.pandas()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:45:59.232135Z","iopub.execute_input":"2025-05-13T22:45:59.232825Z","iopub.status.idle":"2025-05-13T22:45:59.791671Z","shell.execute_reply.started":"2025-05-13T22:45:59.232795Z","shell.execute_reply":"2025-05-13T22:45:59.790890Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load CSV\ndf = pd.read_csv(\"/kaggle/working/blip-vqa_16-16-no-dense-finetuned_predictions.csv\")  # Replace with your CSV file path\n\n# Clean text (optional but recommended)\ndef normalize_text(s):\n    return str(s).strip().lower()\n\ndf['ground_truth_answer'] = df['ground_truth_answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Accuracy\naccuracy = (df['ground_truth_answer'] == df['predicted_answer']).mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['ground_truth_answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['ground_truth_answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# F1 Score (Token-level macro F1)\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['ground_truth_answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(lambda row: sentence_bleu([row['ground_truth_answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn), axis=1)\navg_bleu = df['bleu'].mean()\n\n# # BARTScore (optional; needs large model and GPU)\n# bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n# df['bartscore'] = df.progress_apply(lambda row: bart_scorer.score([row['answer']], [row['ground_truth']])[0], axis=1)\n# avg_bartscore = df['bartscore'].mean()\n\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")\n# print(f\"BARTScore     : {avg_bartscore:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T22:49:50.664598Z","iopub.execute_input":"2025-05-13T22:49:50.664952Z","iopub.status.idle":"2025-05-13T22:49:51.824305Z","shell.execute_reply.started":"2025-05-13T22:49:50.664931Z","shell.execute_reply":"2025-05-13T22:49:51.823587Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3971d204d2240dcbb77d6d1d1818d61"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cf7df4ec833495da76c068a129b6db4"}},"metadata":{}},{"name":"stdout","text":"done in 0.17 seconds, 5745.83 sentences/sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:00<00:00, 64185.10it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 117211.71it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 19398.59it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy      : 0.8520\nBERTScore F1  : 0.9993\nROUGE-L       : 0.8520\nToken F1      : 0.8520\nBLEU          : 0.1515\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Load CSV\ndf = pd.read_csv(\"/kaggle/working/blip_16-16-no-dense-finetuned_predictions.csv\")  # Replace with your CSV file path\n\n# Clean text (optional but recommended)\ndef normalize_text(s):\n    return str(s).strip().lower()\n\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Accuracy\naccuracy = (df['answer'] == df['predicted_answer']).mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# F1 Score (Token-level macro F1)\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn), axis=1)\navg_bleu = df['bleu'].mean()\n\n# # BARTScore (optional; needs large model and GPU)\n# bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n# df['bartscore'] = df.progress_apply(lambda row: bart_scorer.score([row['answer']], [row['ground_truth']])[0], axis=1)\n# avg_bartscore = df['bartscore'].mean()\n\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")\n# print(f\"BARTScore     : {avg_bartscore:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T21:22:03.590280Z","iopub.execute_input":"2025-05-13T21:22:03.590611Z","iopub.status.idle":"2025-05-13T21:22:26.567292Z","shell.execute_reply.started":"2025-05-13T21:22:03.590585Z","shell.execute_reply":"2025-05-13T21:22:26.566547Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be2d86c3c1874fe8afa773bcb6e9d0dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030ba9aa90ae4b31a1a25f3ae3c02e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85fbdc9eefb4421a9a0e22b4e8bd5014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"890a9c0f547b4deb830a6eb73cbf54b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd656d36fe44aeebb6c244e9c84e653"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"981fde4e2a8f440e8d4350150e448f23"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/43 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1723d7ade41b4555a32e5d346689ebff"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/469 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc82f22e37bd4fc0a71462045a920cda"}},"metadata":{}},{"name":"stdout","text":"done in 7.75 seconds, 3869.77 sentences/sec\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30000/30000 [00:01<00:00, 25816.07it/s]\n100%|██████████| 30000/30000 [00:00<00:00, 136600.18it/s]\n100%|██████████| 30000/30000 [00:01<00:00, 21152.51it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy      : 0.7225\nBERTScore F1  : 0.9880\nROUGE-L       : 0.7323\nToken F1      : 0.7227\nBLEU          : 0.1285\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5}]}