{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11472097,"sourceType":"datasetVersion","datasetId":7189537},{"sourceId":11766806,"sourceType":"datasetVersion","datasetId":7387101},{"sourceId":11767512,"sourceType":"datasetVersion","datasetId":7387555},{"sourceId":11772056,"sourceType":"datasetVersion","datasetId":7390719}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T17:27:36.879169Z","iopub.execute_input":"2025-05-12T17:27:36.879377Z","iopub.status.idle":"2025-05-12T17:28:53.220243Z","shell.execute_reply.started":"2025-05-12T17:27:36.879337Z","shell.execute_reply":"2025-05-12T17:28:53.219581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom transformers import BlipProcessor, BlipForQuestionAnswering, BartTokenizer, BartForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n# import evaluate\n# from bert_score import score as bert_score\n\nfrom transformers import (\n    BlipForQuestionAnswering,\n    BlipProcessor\n)\n\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:17:30.819873Z","iopub.execute_input":"2025-05-13T09:17:30.820658Z","iopub.status.idle":"2025-05-13T09:17:32.118806Z","shell.execute_reply.started":"2025-05-13T09:17:30.820630Z","shell.execute_reply":"2025-05-13T09:17:32.118012Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/combined-vqa/flattened_vqa_with_metadata.csv')\ndf = df.sample(frac=1, random_state=42).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:17:40.343709Z","iopub.execute_input":"2025-05-13T09:17:40.344237Z","iopub.status.idle":"2025-05-13T09:17:40.625658Z","shell.execute_reply.started":"2025-05-13T09:17:40.344210Z","shell.execute_reply":"2025-05-13T09:17:40.625071Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# import os\n# import io\n# import torch\n# import warnings\n# from PIL import Image\n# import contextlib\n# import pandas as pd\n\n\n# # Assuming processor and model are already initialized and on CUDA\n# # image_base_path should be the root of your image folder\n# image_base_path = \"/kaggle/input/abo-small/small\"\n\n# skipped = 0\n# skip_logs = []\n\n# for idx, row in df.iterrows():\n#     question = row[\"question\"]\n#     gt_answer = str(row[\"answer\"]).strip().lower()\n#     image_rel_path = row[\"image_path\"]\n\n#     full_path = image_rel_path  # Already contains full relative path\n#     if not os.path.exists(full_path):\n#         skip_logs.append(f\"[SKIP] Image not found: {full_path}\")\n#         skipped += 1\n#         continue\n\n#     try:\n#         image = Image.open(full_path).convert(\"RGB\")\n#     except Exception as e:\n#         skip_logs.append(f\"[SKIP] Failed to load image: {full_path} | Error: {e}\")\n#         skipped += 1\n#         continue\n\n#     with warnings.catch_warnings():\n#         warnings.simplefilter(\"ignore\")\n#         with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n#             inputs = processor(images=image, text=question, return_tensors=\"pt\")\n#             output = model.generate(**inputs, max_new_tokens=10)\n#             pred_answer = processor.decode(output[0], skip_special_tokens=True).strip().lower()\n\n#     print(f\"Q: {question}\\nPredicted: {pred_answer} | Ground Truth: {gt_answer}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:45:26.890893Z","iopub.execute_input":"2025-05-12T16:45:26.891148Z","iopub.status.idle":"2025-05-12T16:45:27.256737Z","shell.execute_reply.started":"2025-05-12T16:45:26.891121Z","shell.execute_reply":"2025-05-12T16:45:27.255983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:45:27.257541Z","iopub.execute_input":"2025-05-12T16:45:27.257817Z","iopub.status.idle":"2025-05-12T16:45:33.709004Z","shell.execute_reply.started":"2025-05-12T16:45:27.257772Z","shell.execute_reply":"2025-05-12T16:45:33.708285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:17:45.671250Z","iopub.execute_input":"2025-05-13T09:17:45.672072Z","iopub.status.idle":"2025-05-13T09:17:48.867999Z","shell.execute_reply.started":"2025-05-13T09:17:45.672045Z","shell.execute_reply":"2025-05-13T09:17:48.867255Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# import torch\n# from peft import get_peft_model, LoraConfig, TaskType\n\n\n\n\n# # LoRA configuration\n# lora_config = LoraConfig(\n#     r=4,\n#     lora_alpha=16,\n#     target_modules=[\"q\", \"v\"],  # 'q' and 'v' are common; confirm exact target modules if needed\n#     lora_dropout=0.1,\n#     bias=\"none\",\n#     task_type=TaskType.SEQ_2_SEQ_LM\n# )\n\n# # Apply LoRA\n# model = get_peft_model(model, lora_config)\n\n# # Print number of trainable parameters\n# def print_trainable_params(model):\n#     trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     total = sum(p.numel() for p in model.parameters())\n#     print(f\"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n\n# print_trainable_params(model)\n\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"projection\", \"value\", \"key\", \"query\", \"fc1\", \"fc2\", \"qkv\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:17:58.151863Z","iopub.execute_input":"2025-05-13T09:17:58.152163Z","iopub.status.idle":"2025-05-13T09:17:58.569247Z","shell.execute_reply.started":"2025-05-13T09:17:58.152137Z","shell.execute_reply":"2025-05-13T09:17:58.568491Z"}},"outputs":[{"name":"stdout","text":"trainable params: 5,898,240 || all params: 390,570,812 || trainable%: 1.5102\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport requests\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport pickle\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Dataset class modification for CSV format\nclass CSVVQADataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, processor):\n        self.data = dataframe\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        image = Image.open(item['image_path']).convert(\"RGB\")\n        text = item['question']\n        answer = item['answer']\n        if isinstance(answer, list):\n            answer = \" \".join(map(str, answer))  # convert list items to str if needed\n        elif not isinstance(answer, str):\n            answer = str(answer)\n        \n        if isinstance(text, list):\n            text = \" \".join(map(str, text))  # convert list items to str if needed\n        elif not isinstance(text, str):\n            text = str(text)\n            \n        encoding = self.processor(\n            image, \n            text, \n            max_length = 64,\n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n        \n        labels = self.processor.tokenizer(\n            answer, \n            max_length= 64, \n            padding=\"max_length\", \n            return_tensors='pt'\n        )\n        \n        encoding[\"labels\"] = labels[\"input_ids\"].squeeze()\n        for k,v in encoding.items():  \n            encoding[k] = v.squeeze()\n        return encoding\n\n# Load and split data\ntrain_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Create datasets\ntrain_dataset = CSVVQADataset(train_df, processor)\nvalid_dataset = CSVVQADataset(valid_df, processor)\n\n# Create dataloaders\nbatch_size = 3\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True,  # Enable shuffling\n    pin_memory=True\n)\nvalid_dataloader = DataLoader(\n    valid_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    pin_memory=True\n)\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nscaler = torch.cuda.amp.GradScaler()\n\n# Training Loop\nmin_eval_loss = float(\"inf\")\ntracking_information = []\npatience = 3\nearly_stopping_hook = 0\n\nfor epoch in range(2):\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\"):\n        for k in batch:\n            batch[k] = batch[k].to(device)\n        with torch.cuda.amp.autocast():\n            outputs = model(**batch)\n            loss = outputs.loss\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        train_loss += loss.item()\n    \n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(valid_dataloader, desc=f\"Epoch {epoch+1} Validation\"):\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(**batch)\n                loss = outputs.loss\n            eval_loss += loss.item()\n    \n    train_loss /= len(train_dataloader)\n    eval_loss /= len(valid_dataloader)\n    tracking_information.append((train_loss, eval_loss))\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Eval Loss = {eval_loss:.4f}\")\n\n    if eval_loss < min_eval_loss:\n        min_eval_loss = eval_loss\n        model.save_pretrained(\"Model/blip-lora-saved\")\n        model.save_pretrained(\"/kaggle/working/blip-lora-saved\")\n        print(\"Model saved.\")\n        early_stopping_hook = 0\n    else:\n        early_stopping_hook += 1\n        if early_stopping_hook >= patience:\n            print(\"⏹Early stopping.\")\n            break\n\npickle.dump(tracking_information, open(\"tracking_info.pkl\", \"wb\"))\nprint(\"🎉 Finetuning complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:18:07.850634Z","iopub.execute_input":"2025-05-13T09:18:07.851438Z","iopub.status.idle":"2025-05-13T18:58:48.057252Z","shell.execute_reply.started":"2025-05-13T09:18:07.851409Z","shell.execute_reply":"2025-05-13T18:58:48.056386Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_101/3676337499.py:85: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\nEpoch 1 Training:   0%|          | 0/42447 [00:00<?, ?it/s]/tmp/ipykernel_101/3676337499.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nEpoch 1 Training: 100%|██████████| 42447/42447 [4:21:40<00:00,  2.70it/s]  \nEpoch 1 Validation:   0%|          | 0/10612 [00:00<?, ?it/s]/tmp/ipykernel_101/3676337499.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1 Validation: 100%|██████████| 10612/10612 [28:54<00:00,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 8.4115, Eval Loss = 8.3859\nModel saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|██████████| 42447/42447 [4:21:06<00:00,  2.71it/s]  \nEpoch 2 Validation: 100%|██████████| 10612/10612 [28:57<00:00,  6.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 8.3376, Eval Loss = 8.3277\nModel saved.\n🎉 Finetuning complete!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n# import evaluate\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\n# from transformers import BartTokenizer, BartForConditionalGeneration\nimport csv\nfrom pathlib import Path\n\n# Prepare output CSV\noutput_path = Path(\"vqa_test_predictions_vilt.csv\")\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load ViLT processor and model\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nmodel = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").to(device)\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/d/sasisnigdhay/combined-vqa/flattened_vqa_with_metadata.csv\")\ndf_sampled = df.sample(frac=0.3, random_state=42).copy()\n\n# Train-test split (80% train, 20% test)\ntrain_df, test_df = train_test_split(df_sampled, test_size=0.2, random_state=42)\ntest_df = test_df.reset_index(drop=True)\n\n# # Load metrics\n# bertscore = evaluate.load(\"bertscore\")\n# bleu = evaluate.load(\"bleu\")\n# rouge = evaluate.load(\"rouge\")\n\n# # Load BART model for BARTScore\n# bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n# bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n\n# def compute_bartscore(preds, refs):\n#     scores = []\n#     for pred, ref in zip(preds, refs):\n#         input_ids = bart_tokenizer.encode(pred, return_tensors=\"pt\").to(device)\n#         with torch.no_grad():\n#             outputs = bart_model.generate(input_ids, max_length=20, num_beams=4)\n#             decoded = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n#             scores.append(1.0 if decoded.lower() == ref.lower() else 0.0)\n#     return scores\n\n# Evaluation on test set\npredictions, references = [], []\ncorrect = 0\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating ViLT on test set\"):\n    try:\n        image = Image.open(row['image_path']).convert(\"RGB\")\n        question = row['question']\n        gt_answer = row['answer'].lower()\n\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            logits = outputs.logits\n            pred_id = logits.argmax(-1).item()\n            pred_answer = model.config.id2label[pred_id].lower()\n\n        predictions.append(pred_answer)\n        references.append(gt_answer)\n\n        if pred_answer == gt_answer:\n            correct += 1\n    except Exception as e:\n        print(f\"Failed on {row['image_path']}: {e}\")\n        predictions.append(\"\")\n        references.append(\"\")\n    with open(output_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        writer.writerow([row['image_path'], question, gt_answer, pred_answer])\n    print(\"predictions saved to csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:44:11.48749Z","iopub.execute_input":"2025-05-12T18:44:11.487663Z","iopub.status.idle":"2025-05-12T18:49:14.572705Z","shell.execute_reply.started":"2025-05-12T18:44:11.487639Z","shell.execute_reply":"2025-05-12T18:49:14.571938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport io\nimport torch\nimport warnings\nfrom PIL import Image\nimport contextlib\nimport pandas as pd\n\n\n# Limit to first 5 rows\ndf = df.head(5)\n\n# Assuming processor and model are already initialized and on CUDA\n# image_base_path should be the root of your image folder\nimage_base_path = \"/kaggle/input/abo-small/small\"\n\nskipped = 0\nskip_logs = []\n\nfor idx, row in df.iterrows():\n    question = row[\"question\"]\n    gt_answer = str(row[\"answer\"]).strip().lower()\n    image_rel_path = row[\"image_path\"]\n\n    full_path = image_rel_path  # Already contains full relative path\n    if not os.path.exists(full_path):\n        skip_logs.append(f\"[SKIP] Image not found: {full_path}\")\n        skipped += 1\n        continue\n\n    try:\n        image = Image.open(full_path).convert(\"RGB\")\n    except Exception as e:\n        skip_logs.append(f\"[SKIP] Failed to load image: {full_path} | Error: {e}\")\n        skipped += 1\n        continue\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n            inputs = processor(images=image, text=question, return_tensors=\"pt\")\n            output = model.generate(**inputs, max_new_tokens=10)\n            pred_answer = processor.decode(output[0], skip_special_tokens=True).strip().lower()\n\n    print(f\"Q: {question}\\nPredicted: {pred_answer} | Ground Truth: {gt_answer}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:50:25.007192Z","iopub.status.idle":"2025-05-12T16:50:25.007515Z","shell.execute_reply.started":"2025-05-12T16:50:25.007366Z","shell.execute_reply":"2025-05-12T16:50:25.007383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets peft accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:13:00.203606Z","iopub.execute_input":"2025-05-12T19:13:00.203769Z","iopub.status.idle":"2025-05-12T19:14:19.805402Z","shell.execute_reply.started":"2025-05-12T19:13:00.203754Z","shell.execute_reply":"2025-05-12T19:14:19.804411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import Dataset, Image\n\n# Load the CSV file\ndf = pd.read_csv(\"/kaggle/input/d/sasisnigdhay/combined-vqa/flattened_vqa_with_metadata.csv\")\ndf = df.sample(frac=0.001, random_state=42).copy()\n\n# Rename columns to match expected format\ndf = df.rename(columns={\"image_path\": \"image\", \"question\": \"question\", \"answer\": \"answer\"})\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\n# Cast the 'image' column to Image type\ndataset = dataset.cast_column(\"image\", Image())\n\n# Split into training and validation sets\nsplit_dataset = dataset.train_test_split(test_size=0.2)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:14:45.484566Z","iopub.execute_input":"2025-05-12T19:14:45.485234Z","iopub.status.idle":"2025-05-12T19:14:49.421622Z","shell.execute_reply.started":"2025-05-12T19:14:45.485202Z","shell.execute_reply":"2025-05-12T19:14:49.421104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# Load processor and model\nmodel_name = \"dandelin/vilt-b32-finetuned-vqa\"\nprocessor = ViltProcessor.from_pretrained(model_name)\nmodel = ViltForQuestionAnswering.from_pretrained(model_name)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\", \"key\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:14:52.632649Z","iopub.execute_input":"2025-05-12T19:14:52.633247Z","iopub.status.idle":"2025-05-12T19:15:26.245674Z","shell.execute_reply.started":"2025-05-12T19:14:52.633221Z","shell.execute_reply":"2025-05-12T19:15:26.244882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    images = [item[\"image\"] for item in batch]\n    questions = [item[\"question\"] for item in batch]\n    answers = [item[\"answer\"] for item in batch]\n\n    # Process inputs\n    encoding = processor(images=images, text=questions, return_tensors=\"pt\", padding=True, truncation=True)\n\n    # Map answers to label IDs\n    label2id = model.config.label2id\n    labels = [label2id.get(ans, label2id[\"unknown\"]) for ans in answers]\n    encoding[\"labels\"] = torch.tensor(labels)\n\n    return encoding\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:15:47.545429Z","iopub.execute_input":"2025-05-12T19:15:47.546158Z","iopub.status.idle":"2025-05-12T19:15:47.551678Z","shell.execute_reply.started":"2025-05-12T19:15:47.546134Z","shell.execute_reply":"2025-05-12T19:15:47.550861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./vilt-lora-vqa\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    learning_rate=5e-5,\n    # evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    fp16=torch.cuda.is_available(),\n    label_names=[\"labels\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=collate_fn,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:15:50.884641Z","iopub.execute_input":"2025-05-12T19:15:50.884913Z","iopub.status.idle":"2025-05-12T19:15:55.803276Z","shell.execute_reply.started":"2025-05-12T19:15:50.884891Z","shell.execute_reply":"2025-05-12T19:15:55.802427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:15:59.543972Z","iopub.execute_input":"2025-05-12T19:15:59.544323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"vilt-lora-vqa\")\nprocessor.save_pretrained(\"vilt-lora-vqa\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
